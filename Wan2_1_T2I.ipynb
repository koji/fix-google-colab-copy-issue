{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNeCxsKJC9k3g27jq5qnn4t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koji/fix-google-colab-copy-issue/blob/main/Wan2_1_T2I.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1B4_GQVxAdiq",
        "outputId": "fd953bd5-caa8-404f-c9a6-e751c7bfbafd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into '/content/ComfyUI'...\n",
            "remote: Enumerating objects: 25271, done.\u001b[K\n",
            "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 25271 (delta 15), reused 4 (delta 4), pack-reused 25248 (from 2)\u001b[K\n",
            "Receiving objects: 100% (25271/25271), 67.17 MiB | 15.71 MiB/s, done.\n",
            "Resolving deltas: 100% (17060/17060), done.\n",
            "Cloning into '/content/ComfyUI/custom_nodes/ComfyUI-GGUF'...\n",
            "remote: Enumerating objects: 702, done.\u001b[K\n",
            "remote: Counting objects: 100% (354/354), done.\u001b[K\n",
            "remote: Compressing objects: 100% (161/161), done.\u001b[K\n",
            "remote: Total 702 (delta 315), reused 199 (delta 193), pack-reused 348 (from 2)\u001b[K\n",
            "Receiving objects: 100% (702/702), 194.70 KiB | 19.47 MiB/s, done.\n",
            "Resolving deltas: 100% (457/457), done.\n",
            "Collecting torchsde\n",
            "  Downloading torchsde-0.2.6-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting gguf\n",
            "  Downloading gguf-0.17.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting av\n",
            "  Downloading av-15.1.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.12/dist-packages (from torchsde) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.12/dist-packages (from torchsde) (1.16.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from torchsde) (2.8.0+cu126)\n",
            "Collecting trampoline>=0.1.2 (from torchsde)\n",
            "  Downloading trampoline-0.1.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from gguf) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from gguf) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->torchsde) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->torchsde) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->torchsde) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->torchsde) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->torchsde) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->torchsde) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->torchsde) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->torchsde) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->torchsde) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->torchsde) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->torchsde) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->torchsde) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->torchsde) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->torchsde) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->torchsde) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->torchsde) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->torchsde) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->torchsde) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->torchsde) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->torchsde) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->torchsde) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->torchsde) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.6.0->torchsde) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.6.0->torchsde) (3.0.2)\n",
            "Downloading torchsde-0.2.6-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gguf-0.17.1-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading av-15.1.0-cp312-cp312-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trampoline-0.1.2-py3-none-any.whl (5.2 kB)\n",
            "Installing collected packages: trampoline, gguf, av, torchsde\n",
            "Successfully installed av-15.1.0 gguf-0.17.1 torchsde-0.2.6 trampoline-0.1.2\n"
          ]
        }
      ],
      "source": [
        "# setup env\n",
        "%cd /content\n",
        "!git clone https://github.com/comfyanonymous/ComfyUI /content/ComfyUI\n",
        "!git clone https://github.com/city96/ComfyUI-GGUF /content/ComfyUI/custom_nodes/ComfyUI-GGUF\n",
        "!pip install torchsde gguf av"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install aria2 -qqy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHSOGB0yAgu8",
        "outputId": "5aaf0a3c-e881-48c6-eed1-278d7d4d765f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following additional packages will be installed:\n",
            "  libaria2-0 libc-ares2\n",
            "The following NEW packages will be installed:\n",
            "  aria2 libaria2-0 libc-ares2\n",
            "0 upgraded, 3 newly installed, 0 to remove and 38 not upgraded.\n",
            "Need to get 1,513 kB of archives.\n",
            "After this operation, 5,441 kB of additional disk space will be used.\n",
            "Selecting previously unselected package libc-ares2:amd64.\n",
            "(Reading database ... 126441 files and directories currently installed.)\n",
            "Preparing to unpack .../libc-ares2_1.18.1-1ubuntu0.22.04.3_amd64.deb ...\n",
            "Unpacking libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.3) ...\n",
            "Selecting previously unselected package libaria2-0:amd64.\n",
            "Preparing to unpack .../libaria2-0_1.36.0-1_amd64.deb ...\n",
            "Unpacking libaria2-0:amd64 (1.36.0-1) ...\n",
            "Selecting previously unselected package aria2.\n",
            "Preparing to unpack .../aria2_1.36.0-1_amd64.deb ...\n",
            "Unpacking aria2 (1.36.0-1) ...\n",
            "Setting up libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.3) ...\n",
            "Setting up libaria2-0:amd64 (1.36.0-1) ...\n",
            "Setting up aria2 (1.36.0-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download models\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/city96/Wan2.1-T2V-14B-gguf/resolve/main/wan2.1-t2v-14b-Q3_K_S.gguf -d /content/ComfyUI/models/unet -o wan2.1-t2v-14b-Q3_K_M.gguf\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/city96/umt5-xxl-encoder-gguf/resolve/main/umt5-xxl-encoder-Q3_K_S.gguf -d /content/ComfyUI/models/clip -o umt5-xxl-encoder-Q3_K_M.gguf\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors -d /content/ComfyUI/models/vae -o/wan_2.1_vae.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/vrgamedevgirl84/Wan14BT2VFusioniX/resolve/main/FusionX_LoRa/Wan2.1_T2V_14B_FusionX_LoRA.safetensors -d /content/ComfyUI/models/loras/FusionX -o Wan2.1_T2V_14B_FusionX_LoRA.safetensors\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMWMesqxAlaQ",
        "outputId": "a0ef5738-bd35-4783-d351-455fc075058b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "600d35|\u001b[1;32mOK\u001b[0m  |   166MiB/s|/content/ComfyUI/models/unet/wan2.1-t2v-14b-Q3_K_M.gguf\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\u001b[0m\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "3ec3e4|\u001b[1;32mOK\u001b[0m  |   150MiB/s|/content/ComfyUI/models/clip/umt5-xxl-encoder-Q3_K_M.gguf\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\u001b[0m\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "b94b44|\u001b[1;32mOK\u001b[0m  |   251MiB/s|/content/ComfyUI/models/vae//wan_2.1_vae.safetensors\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\u001b[0m\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "d9e11c|\u001b[1;32mOK\u001b[0m  |   258MiB/s|/content/ComfyUI/models/loras/FusionX/Wan2.1_T2V_14B_FusionX_LoRA.safetensors\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ComfyUI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwD3wWkaAq3_",
        "outputId": "c846f925-39fc-41ff-9110-900dda02f228"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ComfyUI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random, time\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "from nodes import NODE_CLASS_MAPPINGS\n",
        "from comfy_extras import nodes_hunyuan, nodes_model_advanced\n",
        "import sys\n",
        "import os\n",
        "\n",
        "sys.path.append(\"/content/ComfyUI/custom_nodes/ComfyUI-GGUF\")\n",
        "\n",
        "try:\n",
        "    import importlib.util\n",
        "    spec = importlib.util.spec_from_file_location(\"comfyui_gguf\", \"/content/ComfyUI/custom_nodes/ComfyUI-GGUF/__init__.py\")\n",
        "    comfyui_gguf = importlib.util.module_from_spec(spec)\n",
        "    sys.modules[\"comfyui_gguf\"] = comfyui_gguf\n",
        "    spec.loader.exec_module(comfyui_gguf)\n",
        "\n",
        "    if hasattr(comfyui_gguf, 'NODE_CLASS_MAPPINGS'):\n",
        "        NODE_CLASS_MAPPINGS.update(comfyui_gguf.NODE_CLASS_MAPPINGS)\n",
        "        print(\"ComfyUI-GGUF nodes loaded successfully!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading ComfyUI-GGUF: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRkDz6_eAtx8",
        "outputId": "934851b0-5b51-4edb-eff3-6ebb699cd2c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ComfyUI-GGUF nodes loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gguf_nodes = [k for k in NODE_CLASS_MAPPINGS.keys() if \"GGUF\" in k]\n",
        "print(f\"Available GGUF nodes: {gguf_nodes}\")\n",
        "\n",
        "required_nodes = [\"UnetLoaderGGUF\", \"CLIPLoaderGGUF\", \"LoraLoaderModelOnly\", \"VAELoader\"]\n",
        "missing_nodes = [node for node in required_nodes if node not in NODE_CLASS_MAPPINGS]\n",
        "\n",
        "if missing_nodes:\n",
        "    print(f\"Missing nodes: {missing_nodes}\")\n",
        "    print(\"Available nodes:\", list(NODE_CLASS_MAPPINGS.keys())[:20])\n",
        "else:\n",
        "    UnetLoaderGGUF = NODE_CLASS_MAPPINGS[\"UnetLoaderGGUF\"]()\n",
        "    CLIPLoaderGGUF = NODE_CLASS_MAPPINGS[\"CLIPLoaderGGUF\"]()\n",
        "    LoraLoaderModelOnly = NODE_CLASS_MAPPINGS[\"LoraLoaderModelOnly\"]()\n",
        "    VAELoader = NODE_CLASS_MAPPINGS[\"VAELoader\"]()\n",
        "\n",
        "    CLIPTextEncode = NODE_CLASS_MAPPINGS[\"CLIPTextEncode\"]()\n",
        "    EmptyHunyuanLatentVideo = nodes_hunyuan.NODE_CLASS_MAPPINGS[\"EmptyHunyuanLatentVideo\"]()\n",
        "\n",
        "    KSampler = NODE_CLASS_MAPPINGS[\"KSampler\"]()\n",
        "    ModelSamplingSD3 = nodes_model_advanced.NODE_CLASS_MAPPINGS[\"ModelSamplingSD3\"]()\n",
        "    VAEDecode = NODE_CLASS_MAPPINGS[\"VAEDecode\"]()\n",
        "\n",
        "    print(\"All required nodes loaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdNjxpmaA3NZ",
        "outputId": "098fa01d-a016-47c9-dbc2-28a2a026de42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available GGUF nodes: ['UnetLoaderGGUF', 'CLIPLoaderGGUF', 'DualCLIPLoaderGGUF', 'TripleCLIPLoaderGGUF', 'QuadrupleCLIPLoaderGGUF', 'UnetLoaderGGUFAdvanced']\n",
            "All required nodes loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    model_paths = {\n",
        "        \"unet\": \"/content/ComfyUI/models/unet/wan2.1-t2v-14b-Q3_K_M.gguf\",\n",
        "        \"clip\": \"/content/ComfyUI/models/clip/umt5-xxl-encoder-Q3_K_M.gguf\",\n",
        "        \"vae\": \"/content/ComfyUI/models/vae/wan_2.1_vae.safetensors\",\n",
        "        \"lora\": \"/content/ComfyUI/models/loras/FusionX/Wan2.1_T2V_14B_FusionX_LoRA.safetensors\"\n",
        "    }\n",
        "\n",
        "    print(\"\\nModel file check:\")\n",
        "    for model_type, path in model_paths.items():\n",
        "        exists = os.path.exists(path)\n",
        "        print(f\"{model_type}: {'‚úì' if exists else '‚úó'} {path}\")\n",
        "\n",
        "    try:\n",
        "        with torch.inference_mode():\n",
        "            print(\"\\nLoading models...\")\n",
        "\n",
        "            unet = UnetLoaderGGUF.load_unet(\"wan2.1-t2v-14b-Q3_K_M.gguf\")[0]\n",
        "            print(\"‚úì UNet loaded\")\n",
        "\n",
        "            clip = CLIPLoaderGGUF.load_clip(\"umt5-xxl-encoder-Q3_K_M.gguf\", \"wan\")[0]\n",
        "            print(\"‚úì CLIP loaded\")\n",
        "\n",
        "            lora = LoraLoaderModelOnly.load_lora_model_only(unet, \"FusionX/Wan2.1_T2V_14B_FusionX_LoRA.safetensors\", 1.0)[0]\n",
        "            print(\"‚úì LoRA loaded\")\n",
        "\n",
        "            vae = VAELoader.load_vae(\"wan_2.1_vae.safetensors\")[0]\n",
        "            print(\"‚úì VAE loaded\")\n",
        "\n",
        "            print(\"\\n&#x1f389; All models loaded successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n&#x274c; Error loading models: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ipu5wRyA30T",
        "outputId": "c5a9d2ab-be87-467d-8b22-9a6dac07eaa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model file check:\n",
            "unet: ‚úì /content/ComfyUI/models/unet/wan2.1-t2v-14b-Q3_K_M.gguf\n",
            "clip: ‚úì /content/ComfyUI/models/clip/umt5-xxl-encoder-Q3_K_M.gguf\n",
            "vae: ‚úì /content/ComfyUI/models/vae/wan_2.1_vae.safetensors\n",
            "lora: ‚úì /content/ComfyUI/models/loras/FusionX/Wan2.1_T2V_14B_FusionX_LoRA.safetensors\n",
            "\n",
            "Loading models...\n",
            "‚úì UNet loaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Dequantizing token_embd.weight to prevent runtime OOM.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì CLIP loaded\n",
            "‚úì LoRA loaded\n",
            "‚úì VAE loaded\n",
            "\n",
            "&#x1f389; All models loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_image(positive_prompt, negative_prompt,\n",
        "                  seed=0, steps=10, cfg=1.0,\n",
        "                  sampler_name=\"euler\", scheduler=\"beta\",\n",
        "                  width=1280, height=720, length=1,\n",
        "                  output_path=\"/content/test.png\"):\n",
        "    \"\"\"\n",
        "    High-Quality Character Image Generation Function\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    positive_prompt : str\n",
        "        Positive prompt (detailed description of desired content)\n",
        "    negative_prompt : str\n",
        "        Negative prompt (specification of elements to avoid)\n",
        "    seed : int\n",
        "        Random seed (0 for random generation)\n",
        "    steps : int\n",
        "        Number of sampling steps (affects image quality)\n",
        "    cfg : float\n",
        "        CFG value (prompt adherence strength)\n",
        "    sampler_name : str\n",
        "        Name of the sampler\n",
        "    scheduler : str\n",
        "        Name of the scheduler\n",
        "    width, height : int\n",
        "        Output image dimensions\n",
        "    length : int\n",
        "        Parameter for video generation (set to 1 for still images)\n",
        "    output_path : str\n",
        "        Path to save the generated image\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    PIL.Image\n",
        "        The generated image object\n",
        "    \"\"\"\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        positive_encoded = CLIPTextEncode.encode(clip, positive_prompt)[0]\n",
        "        negative_encoded = CLIPTextEncode.encode(clip, negative_prompt)[0]\n",
        "\n",
        "        model_configured = ModelSamplingSD3.patch(lora, 1.0)[0]\n",
        "        latent_space = EmptyHunyuanLatentVideo.generate(width, height, length)[0]\n",
        "\n",
        "        if seed == 0:\n",
        "            random.seed(int(time.time()))\n",
        "            seed = random.randint(0, 18446744073709551615)\n",
        "\n",
        "        print(f\"üé≤ Seed used: {seed}\")\n",
        "\n",
        "        samples = KSampler.sample(\n",
        "            model_configured, seed, steps, cfg,\n",
        "            sampler_name, scheduler,\n",
        "            positive_encoded, negative_encoded, latent_space\n",
        "        )[0]\n",
        "\n",
        "        decoded = VAEDecode.decode(vae, samples)[0].detach()\n",
        "        image_array = np.array(decoded * 255, dtype=np.uint8)[0]\n",
        "        final_image = Image.fromarray(image_array)\n",
        "\n",
        "        if output_path:\n",
        "            final_image.save(output_path)\n",
        "            print(f\"üíæ Image saved to: {output_path}\")\n",
        "\n",
        "        return final_image"
      ],
      "metadata": {
        "id": "RvUxVsIUA8gY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random, time\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add ComfyUI directory to the Python path\n",
        "sys.path.append(\"/content/ComfyUI\")\n",
        "\n",
        "from nodes import NODE_CLASS_MAPPINGS\n",
        "from comfy_extras import nodes_hunyuan, nodes_model_advanced\n",
        "\n",
        "\n",
        "sys.path.append(\"/content/ComfyUI/custom_nodes/ComfyUI-GGUF\")\n",
        "\n",
        "try:\n",
        "    import importlib.util\n",
        "    spec = importlib.util.spec_from_file_location(\"comfyui_gguf\", \"/content/ComfyUI/custom_nodes/ComfyUI-GGUF/__init__.py\")\n",
        "    comfyui_gguf = importlib.util.module_from_spec(spec)\n",
        "    sys.modules[\"comfyui_gguf\"] = comfyui_gguf\n",
        "    spec.loader.exec_module(comfyui_gguf)\n",
        "\n",
        "    if hasattr(comfyui_gguf, 'NODE_CLASS_MAPPINGS'):\n",
        "        NODE_CLASS_MAPPINGS.update(comfyui_gguf.NODE_CLASS_MAPPINGS)\n",
        "        print(\"ComfyUI-GGUF nodes loaded successfully!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading ComfyUI-GGUF: {e}\")\n",
        "\n",
        "gguf_nodes = [k for k in NODE_CLASS_MAPPINGS.keys() if \"GGUF\" in k]\n",
        "print(f\"Available GGUF nodes: {gguf_nodes}\")\n",
        "\n",
        "required_nodes = [\"UnetLoaderGGUF\", \"CLIPLoaderGGUF\", \"LoraLoaderModelOnly\", \"VAELoader\"]\n",
        "missing_nodes = [node for node in required_nodes if node not in NODE_CLASS_MAPPINGS]\n",
        "\n",
        "if missing_nodes:\n",
        "    print(f\"Missing nodes: {missing_nodes}\")\n",
        "    print(\"Available nodes:\", list(NODE_CLASS_MAPPINGS.keys())[:20])\n",
        "else:\n",
        "    UnetLoaderGGUF = NODE_CLASS_MAPPINGS[\"UnetLoaderGGUF\"]()\n",
        "    CLIPLoaderGGUF = NODE_CLASS_MAPPINGS[\"CLIPLoaderGGUF\"]()\n",
        "    LoraLoaderModelOnly = NODE_CLASS_MAPPINGS[\"LoraLoaderModelOnly\"]()\n",
        "    VAELoader = NODE_CLASS_MAPPINGS[\"VAELoader\"]()\n",
        "\n",
        "    CLIPTextEncode = NODE_CLASS_MAPPINGS[\"CLIPTextEncode\"]()\n",
        "    EmptyHunyuanLatentVideo = nodes_hunyuan.NODE_CLASS_MAPPINGS[\"EmptyHunyuanLatentVideo\"]()\n",
        "\n",
        "    KSampler = NODE_CLASS_MAPPINGS[\"KSampler\"]()\n",
        "    ModelSamplingSD3 = nodes_model_advanced.NODE_CLASS_MAPPINGS[\"ModelSamplingSD3\"]()\n",
        "    VAEDecode = NODE_CLASS_MAPPINGS[\"VAEDecode\"]()\n",
        "\n",
        "    print(\"All required nodes loaded successfully!\")\n",
        "\n",
        "\n",
        "model_paths = {\n",
        "    \"unet\": \"/content/ComfyUI/models/unet/wan2.1-t2v-14b-Q3_K_M.gguf\",\n",
        "    \"clip\": \"/content/ComfyUI/models/clip/umt5-xxl-encoder-Q3_K_M.gguf\",\n",
        "    \"vae\": \"/content/ComfyUI/models/vae/wan_2.1_vae.safetensors\",\n",
        "    \"lora\": \"/content/ComfyUI/models/loras/FusionX/Wan2.1_T2V_14B_FusionX_LoRA.safetensors\"\n",
        "}\n",
        "\n",
        "print(\"\\nModel file check:\")\n",
        "for model_type, path in model_paths.items():\n",
        "    exists = os.path.exists(path)\n",
        "    print(f\"{model_type}: {'‚úì' if exists else '‚úó'} {path}\")\n",
        "\n",
        "try:\n",
        "    with torch.inference_mode():\n",
        "        print(\"\\nLoading models...\")\n",
        "\n",
        "        unet = UnetLoaderGGUF.load_unet(\"wan2.1-t2v-14b-Q3_K_M.gguf\")[0]\n",
        "        print(\"‚úì UNet loaded\")\n",
        "\n",
        "        clip = CLIPLoaderGGUF.load_clip(\"umt5-xxl-encoder-Q3_K_M.gguf\", \"wan\")[0]\n",
        "        print(\"‚úì CLIP loaded\")\n",
        "\n",
        "        lora = LoraLoaderModelOnly.load_lora_model_only(unet, \"FusionX/Wan2.1_T2V_14B_FusionX_LoRA.safetensors\", 1.0)[0]\n",
        "        print(\"‚úì LoRA loaded\")\n",
        "\n",
        "        vae = VAELoader.load_vae(\"wan_2.1_vae.safetensors\")[0]\n",
        "        print(\"‚úì VAE loaded\")\n",
        "\n",
        "        print(\"\\n&#x1f389; All models loaded successfully!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n&#x274c; Error loading models: {e}\")\n",
        "\n",
        "def generate_image(positive_prompt, negative_prompt,\n",
        "                  seed=0, steps=10, cfg=1.0,\n",
        "                  sampler_name=\"euler\", scheduler=\"beta\",\n",
        "                  width=1280, height=720, length=1,\n",
        "                  output_path=\"/content/test.png\"):\n",
        "    \"\"\"\n",
        "    High-Quality Character Image Generation Function\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    positive_prompt : str\n",
        "        Positive prompt (detailed description of desired content)\n",
        "    negative_prompt : str\n",
        "        Negative prompt (specification of elements to avoid)\n",
        "    seed : int\n",
        "        Random seed (0 for random generation)\n",
        "    steps : int\n",
        "        Number of sampling steps (affects image quality)\n",
        "    cfg : float\n",
        "        CFG value (prompt adherence strength)\n",
        "    sampler_name : str\n",
        "        Name of the sampler\n",
        "    scheduler : str\n",
        "        Name of the scheduler\n",
        "    width, height : int\n",
        "        Output image dimensions\n",
        "    length : int\n",
        "        Parameter for video generation (set to 1 for still images)\n",
        "    output_path : str\n",
        "        Path to save the generated image\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    PIL.Image\n",
        "        The generated image object\n",
        "    \"\"\"\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        positive_encoded = CLIPTextEncode.encode(clip, positive_prompt)[0]\n",
        "        negative_encoded = CLIPTextEncode.encode(clip, negative_prompt)[0]\n",
        "\n",
        "        model_configured = ModelSamplingSD3.patch(lora, 1.0)[0]\n",
        "        latent_space = EmptyHunyuanLatentVideo.generate(width, height, length)[0]\n",
        "\n",
        "        if seed == 0:\n",
        "            random.seed(int(time.time()))\n",
        "            seed = random.randint(0, 18446744073709551615)\n",
        "\n",
        "        print(f\"üé≤ Seed used: {seed}\")\n",
        "\n",
        "        samples = KSampler.sample(\n",
        "            model_configured, seed, steps, cfg,\n",
        "            sampler_name, scheduler,\n",
        "            positive_encoded, negative_encoded, latent_space\n",
        "        )[0]\n",
        "\n",
        "        decoded = VAEDecode.decode(vae, samples)[0].detach()\n",
        "        image_array = np.array(decoded * 255, dtype=np.uint8)[0]\n",
        "        final_image = Image.fromarray(image_array)\n",
        "\n",
        "        if output_path:\n",
        "            final_image.save(output_path)\n",
        "            print(f\"üíæ Image saved to: {output_path}\")\n",
        "\n",
        "        return final_image\n",
        "\n",
        "positive_test_prompt = \"\"\"\n",
        "A vibrant rhythm dance game screenshot featuring the 3D animated character from the reference photo, keeping its unique style, hat, outfit, and confident dance pose. Immersive cinematic lighting with neon pink and purple glow, glossy reflective dance floor shining under spotlights, and dynamic 3D cartoon style. Rhythm game interface with immersive UI: score meter at the top, colorful music waveform animations synced to the beat, stage timer countdown, and floating combo numbers. Highly detailed, game-like atmosphere with energy bars, neon particle effects, and immersive arcade rhythm game HUD elements. Ultra-detailed, cinematic, immersive, 3D animation.\n",
        "\"\"\"\n",
        "\n",
        "negative_test_prompt = \"\"\"\n",
        "Realistic proportions, scary, creepy, ugly, deformed, extra limbs, blurry, low resolution, messy background, dark horror style, grotesque, overly sexualized, distorted anatomy, bad hands, bad face, text, watermark\n",
        "\"\"\"\n",
        "\n",
        "test_image = generate_image(\n",
        "    positive_test_prompt,\n",
        "    negative_test_prompt,\n",
        "    output_path=\"/content/test.png\"\n",
        ")\n",
        "\n",
        "display(test_image)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "uvPpLlM3BQnk",
        "outputId": "51bac9f3-fa99-410e-df80-8076667b5ae8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'nodes'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4137386084.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnodes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNODE_CLASS_MAPPINGS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcomfy_extras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnodes_hunyuan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes_model_advanced\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nodes'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Oo5_MgNOCCWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZqaNCBkJD2Zu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}